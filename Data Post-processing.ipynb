{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEIwXB8cQ4z8",
        "outputId": "fabd9826-7f6e-4721-d5db-0fab9b3f3f7e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def token_file_fix(filename):\n",
        "    token_data = open(filename, 'r').read()\n",
        "\n",
        "    all_tokens = re.findall(r'Record\\n##ID[\\n#\\d-]+', token_data)\n",
        "    for single_token in all_tokens:\n",
        "        pattern = single_token.splitlines()\n",
        "        pattern[-2:] = map(lambda x: '##'+x, pattern[-2:])\n",
        "        token_data = token_data.replace(single_token, '\\n'.join(pattern)+'\\n')\n",
        "    open(filename, 'w').write(token_data)\n",
        "\n",
        "filename = \"/content/drive/MyDrive/n2c2/trainingdata_v3/60/token_test_1.txt\"\n",
        "token_file_fix(filename)"
      ],
      "metadata": {
        "id": "wHWzp5PbnbDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9TVjeMCgNM0T"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def detokenize(pred_token_test_path, pred_label_test_path):\n",
        "    \"\"\"\n",
        "    convert suub-word level BioBERT-NER results to full words and labels.\n",
        "\n",
        "    Args:\n",
        "        pred_token_test_path: path to token_test.txt from output folder. ex) output/token_test.txt\n",
        "        pred_label_test_path: path to label_test.txt from output folder. ex) output/label_test.txt\n",
        "    Outs:\n",
        "        A dictionary that contains full words and predicted labels.\n",
        "    \"\"\"\n",
        "\n",
        "    # read predicted\n",
        "    pred = {'toks': [], 'labels': []}  # dictionary for predicted tokens and labels.\n",
        "    with open(pred_token_test_path, 'r') as in_tok, open(pred_label_test_path, 'r') as in_lab:  # 'token_test.txt'\n",
        "        for lineIdx, (lineTok, lineLab) in enumerate(zip(in_tok, in_lab)):\n",
        "            lineTok = lineTok.strip()\n",
        "            pred['toks'].append(lineTok)\n",
        "\n",
        "            lineLab = lineLab.strip()\n",
        "            if lineLab in ['[CLS]', '[SEP]', 'X']:  # replace non-text tokens with O. These will not be evaluated.\n",
        "                pred['labels'].append('O')\n",
        "                continue\n",
        "            pred['labels'].append(lineLab)\n",
        "\n",
        "    assert (len(pred['toks']) == len(\n",
        "        pred['labels'])), \"Error! : len(pred['toks'])(%s) != len(pred['labels'])(%s) : Please report us \" % (\n",
        "    len(pred['toks']), len(pred['labels']))\n",
        "\n",
        "    bert_pred = {'toks': [], 'labels': [], 'sentence': []}\n",
        "    buf = []\n",
        "    for t, l in zip(pred['toks'], pred['labels']):\n",
        "        if t in ['[CLS]', '[SEP]']:  # non-text tokens will not be evaluated.\n",
        "            bert_pred['toks'].append(t)\n",
        "            bert_pred['labels'].append(t)  # Tokens and labels should be identical if they are [CLS] or [SEP]\n",
        "            if t == '[SEP]':\n",
        "                bert_pred['sentence'].append(buf)\n",
        "                buf = []\n",
        "            continue\n",
        "        elif t[:2] == '##':  # if it is a piece of a word (broken by Word Piece tokenizer)\n",
        "            bert_pred['toks'][-1] += t[2:]  # append pieces to make a full-word\n",
        "            buf[-1] += t[2:]\n",
        "        else:\n",
        "            bert_pred['toks'].append(t)\n",
        "            bert_pred['labels'].append(l)\n",
        "            buf.append(t)\n",
        "\n",
        "    assert (len(bert_pred['toks']) == len(bert_pred['labels'])), (\n",
        "        \"Error! : len(bert_pred['toks']) != len(bert_pred['labels']) : Please report us\")\n",
        "\n",
        "    return bert_pred\n",
        "def Create_predict_annotation(data_doc_dir, tokens, tag_pred, output_dir):\n",
        "\n",
        "    predict_annotation = [];\n",
        "    Labels = [\"B-Disposition\", \"I-Disposition\", \"B-NoDisposition\", \"I-NoDisposition\", \"B-Undetermined\", \"I-Undetermined\"]\n",
        "    Record_ID_Flag = \"RecordID\";\n",
        "    num_predictions = len(tokens);\n",
        "\n",
        "    for token_index in range(num_predictions):\n",
        "        #Check to see which record predictions are from\n",
        "        if  type(tokens[token_index]) == str and  Record_ID_Flag in tokens[token_index]:\n",
        "            ID_token = tokens[token_index];\n",
        "\n",
        "            Record_ID = ID_token[8:];\n",
        "            Record_path = data_doc_dir + Record_ID + \".txt\";\n",
        "            Record = open(Record_path, 'r').read();\n",
        "\n",
        "            Output_path = output_dir + Record_ID + \".ann\";\n",
        "            Output_ann = open(Output_path, 'w');\n",
        "\n",
        "            Term_index = 1;\n",
        "            token_record_start_index = 0;\n",
        "            token_record_end_index = 0;\n",
        "\n",
        "        if tag_pred[token_index][0] == \"B\":\n",
        "            entity = tokens[token_index];\n",
        "            token_record_start_index = Record.index(tokens[token_index], token_record_end_index);\n",
        "            token_record_end_index = token_record_start_index + len(tokens[token_index]);\n",
        "            if tag_pred[token_index + 1] == \"I\":\n",
        "                continue;\n",
        "        elif tag_pred[token_index][0] == \"I\":\n",
        "            entity = entity + \" \" + tokens[token_index];\n",
        "            #add one to account for space between words\n",
        "            token_record_end_index += (1 + len(tokens[token_index]));\n",
        "            if tag_pred[token_index + 1][0] == \"I\":\n",
        "                continue;\n",
        "        else: continue;\n",
        "\n",
        "        Annotation = \"T\" + str(Term_index) + \"\\t\" + \"Drug \" + str(token_record_start_index) + \" \" + str(token_record_end_index) + \"\\t\" + entity + \"\\n\";\n",
        "        Annotation += \"E\" + str(Term_index) + \"\\t\" +str(tag_pred[token_index].split('-')[-1]) +':T'+ str(Term_index) + \"\\n\";\n",
        "        Output_ann.write(Annotation);\n",
        "        #predict_annotation.append(Annotation);\n",
        "        Term_index += 1;\n",
        "\n",
        "    return 0;\n",
        "\n",
        "def main():\n",
        "    #path containing predictions\n",
        "    predicition_dir = \"/content/drive/MyDrive/n2c2/trainingdata_v3/60/\";\n",
        "    #path with correct answers\n",
        "    test_data_dir = \"/content/drive/MyDrive/n2c2/trainingdata_v3/trainingdata_v3/dev/\";\n",
        "    #path for annotated predictions\n",
        "    output_dir = predicition_dir + \"ann_pred/\";\n",
        "\n",
        "    predict_tokens_path = predicition_dir + \"token_test_1.txt\";\n",
        "    predict_labels_path = predicition_dir + \"label_test_1.txt\";\n",
        "    bert_pred = detokenize(predict_tokens_path, predict_labels_path);\n",
        "    tokens = bert_pred[\"toks\"];\n",
        "    labels = bert_pred[\"labels\"];\n",
        "\n",
        "\n",
        "    Create_predict_annotation(test_data_dir, tokens, labels, output_dir);\n",
        "\n",
        "\n",
        "\n",
        "    return 0;\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()      "
      ]
    }
  ]
}