{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6 (1).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCo5kU44w-gh",
        "outputId": "86baf1c1-706f-48c0-9ac1-2d8e06910d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UH2Jt0Qfw5W2",
        "outputId": "e9854443-2343-424c-e85c-813b4f085965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "-------Training Entities Summary--------\n",
            "Total Entites: 6196\n",
            "Entities Captured: 7207\n",
            "Percent Captured: 1.16\n",
            "-------Eval Entities Summary--------\n",
            "Total Entites: 1033\n",
            "Entities Captured: 1153\n",
            "Percent Captured: 1.12\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import argparse\n",
        "\n",
        "default_working_dir = \"/content/drive/MyDrive/n2c2/trainingdata_v3/trainingdata_v3/\"\n",
        "parser = argparse.ArgumentParser();\n",
        "parser.add_argument('--max_seq_length', type=int, default=128);\n",
        "parser.add_argument('--working_dir', type=str, default= default_working_dir)\n",
        "parser.add_argument('-f')\n",
        "args = parser.parse_args();\n",
        "\n",
        "#global variable\n",
        "Entity_Count = 0;\n",
        "Entities_Captured = 0;\n",
        "\n",
        "def Parse_Annotation(filepath = \"\"):\n",
        "    Annotation = open(filepath, \"r\");\n",
        "    Annotation = Annotation.readlines();\n",
        "    global Entity_Count;\n",
        "\n",
        "    Ann_List = [];\n",
        "    remove_list = list(string.punctuation);\n",
        "    remove_list.remove(\";\");\n",
        "    for Ann in Annotation:\n",
        "        for punct in remove_list:\n",
        "            Ann = Ann.replace(punct, \" \");\n",
        "        Ann = Ann.replace(\".\", \"\");\n",
        "\n",
        "        if Ann[0] == \"T\":\n",
        "            Entity_Count+= 1;\n",
        "            Ann = nltk.tokenize.word_tokenize(Ann);\n",
        "            if len(Ann) > 5 and Ann.count(\";\")>0:\n",
        "                Ann = [int(Ann[2]), Ann[1]] + Ann[7:];\n",
        "                if len(Ann) > 3:\n",
        "                    BAnn = tuple(Ann[:3]);\n",
        "                    Ann_List.append(BAnn);\n",
        "                    Term_index = Ann[0] + 1;\n",
        "                    for IAnn in Ann[3:]:\n",
        "                        tag = \"I\" + Ann[1];\n",
        "                        IAnn = (Term_index, tag, IAnn);\n",
        "                        Term_index = Term_index + 1;\n",
        "                        Ann_List.append(IAnn);\n",
        "                else:\n",
        "                    Ann = tuple(Ann);\n",
        "                    Ann_List.append(Ann);\n",
        "            else:\n",
        "                Ann = [int(Ann[2]), Ann[1]] + Ann[4:];\n",
        "                if len(Ann) > 3:\n",
        "                    BAnn = tuple(Ann[:3]);\n",
        "                    Ann_List.append(BAnn);\n",
        "                    Term_index = Ann[0] + 1;\n",
        "                    for IAnn in Ann[3:]:\n",
        "                        tag = \"I\" + Ann[1];\n",
        "                        IAnn = (Term_index, tag, IAnn);\n",
        "                        Term_index = Term_index + 1;\n",
        "                        Ann_List.append(IAnn);\n",
        "                else:\n",
        "                    Ann = tuple(Ann);\n",
        "                    Ann_List.append(Ann);\n",
        "\n",
        "    Term_index = []\n",
        "    Term_dict = {};\n",
        "    [Term_index.append(x[0]) for x in Ann_List]\n",
        "    Term_index.sort();\n",
        "    for Terms in Term_index:\n",
        "        Term_dict[Terms] = None;\n",
        "    for tuples in Ann_List:\n",
        "        Term_dict[tuples[0]] = tuples[1:];\n",
        "\n",
        "    Ann_List = list(Term_dict.values());\n",
        "    Ann_len = len(Ann_List);\n",
        "    #Entity_Count += Ann_len;\n",
        "    return Ann_List;\n",
        "\n",
        "def Tokenize_EHR(file_path = \"\"):\n",
        "    EHR_Tokenized = [];\n",
        "    EHR_txt = open(file_path, 'r').read();\n",
        "    # remove specific characters from text\n",
        "    punctuation = list(string.punctuation);\n",
        "    punctuation.remove('.');\n",
        "    # punctuation.remove('?');\n",
        "    # punctuation.remove('!');\n",
        "    for punct in punctuation:\n",
        "        EHR_txt = EHR_txt.replace(punct, \" \")\n",
        "    #Split text by paragraph\n",
        "    EHR_Sections = EHR_txt.split(\"\\n\\n\\n\")\n",
        "\n",
        "    #split section by sentances(sentace indicators: ., ?, !)\n",
        "    for section in EHR_Sections:\n",
        "        if(section == \"\"): continue;\n",
        "        else:\n",
        "            # sentances = section.split(\".\");\n",
        "            # sentances = section.split(\"!\");\n",
        "            # sentances = section.split(\"?\");\n",
        "            sentances = nltk.sent_tokenize(section);\n",
        "            # split sentances into tokens\n",
        "            for sent in sentances:\n",
        "                sent = sent.replace(\".\", \" \")\n",
        "                # sent = sent.replace(\"!\", \" \")\n",
        "                # sent = sent.replace(\"?\", \" \")\n",
        "                sent_tokens = nltk.word_tokenize(sent);\n",
        "                EHR_Tokenized.append(sent_tokens);\n",
        "            # for sent in sentances:\n",
        "            #     sent = sent.replace(\"?\", \" \")\n",
        "            #     sent_tokens = nltk.word_tokenize(sent);\n",
        "            #     EHR_Tokenized.append(sent_tokens);\n",
        "            # for sent in sentances:\n",
        "            #     sent = sent.replace(\"!\", \" \")\n",
        "            #     sent_tokens = nltk.word_tokenize(sent);\n",
        "            #     EHR_Tokenized.append(sent_tokens);\n",
        "\n",
        "\n",
        "    return EHR_Tokenized;\n",
        "\n",
        "def Generate_cmed_data(EHR_data_dir = \"\", EHR_fileID = \"\", write_file = False, max_seq_length = 128):\n",
        "\n",
        "    Tags = {\"Disposition\":(\"B-Disposition\"), \"IDisposition\":(\"I-Disposition\"), \"NoDisposition\":(\"B-NoDisposition\"),\n",
        "           \"INoDisposition\":(\"I-NoDisposition\"), \"Undetermined\":(\"B-Undetermined\"), \"IUndetermined\":(\"I-Undetermined\"), \"Outside\":{\"O\"}};\n",
        "\n",
        "\n",
        "    EHR_Ann_filepath = EHR_data_dir + EHR_fileID + \".ann\";\n",
        "    EHR_doc_filepath = EHR_data_dir + EHR_fileID + \".txt\";\n",
        "    EHR_Tagged_filepath = EHR_data_dir + EHR_fileID + \"_Tagged.txt\";\n",
        "\n",
        "    EHR_Tokens = Tokenize_EHR(EHR_doc_filepath);\n",
        "    Parsed_Annotation = Parse_Annotation(EHR_Ann_filepath);\n",
        "    Parsed_Annotation.append((\"[END]\", \"[END]\"));\n",
        "\n",
        "    Tagged_EHR_List = [];\n",
        "\n",
        "    if(write_file) :Tagged_EHR_File = open(EHR_Tagged_filepath, \"w\");\n",
        "    Annotation_index = 0;\n",
        "\n",
        "    stop_words = set(stopwords.words('english'));\n",
        "    removal_list = list(stop_words);\n",
        "\n",
        "    EHR_ID_Token = \"RecordID\" + str(EHR_fileID);\n",
        "    Tagged_EHR_List.append((EHR_ID_Token, 'O'));\n",
        "    Tagged_EHR_List.append((\"\",\"\"));\n",
        "\n",
        "    global Entities_Captured;\n",
        "    for sents in EHR_Tokens:\n",
        "        seq_index = 0;\n",
        "        if(len(sents) == 1) and (Parsed_Annotation[Annotation_index][1] not in sents): continue;\n",
        "        for words in sents:\n",
        "           #Make specific changes relating to dataset here\n",
        "           if Parsed_Annotation[Annotation_index][1] in removal_list:removal_list.remove(Parsed_Annotation[Annotation_index][1])\n",
        "           elif(words in removal_list): continue;\n",
        "           elif(\"___\" in words): continue;\n",
        "           #elif Parsed_Annotation[Annotation_index][1] in removal_list:\n",
        "           #    Annotation_index+= 1;\n",
        "           #    Entities_Captured += 1;\n",
        "\n",
        "           if(Parsed_Annotation[Annotation_index][1] in words) or (len(words) > 3 and words in Parsed_Annotation[Annotation_index][1]):\n",
        "               Entities_Captured += 1;\n",
        "               tag = Tags[Parsed_Annotation[Annotation_index][0]];\n",
        "               Tagged_EHR_List.append((words, tag))\n",
        "               seq_index += 1;\n",
        "               if(seq_index == max_seq_length):\n",
        "                   Tagged_EHR_List.append((\"\", \"\"));\n",
        "                   seq_index = 0;\n",
        "               if(write_file): Tagged_EHR_File.write(words + \" \" + tag + \"\\n\");\n",
        "               Annotation_index = Annotation_index + 1;\n",
        "\n",
        "           else:\n",
        "               Tagged_EHR_List.append((words, 'O'));\n",
        "               seq_index += 1;\n",
        "               if(seq_index == max_seq_length):\n",
        "                   Tagged_EHR_List.append((\"\", \"\"));\n",
        "                   seq_index = 0;\n",
        "               if(write_file):Tagged_EHR_File.write(words + \" \" + '0' + \"\\n\");\n",
        "\n",
        "        if(seq_index>0):Tagged_EHR_List.append((\"\", \"\"));\n",
        "        if(write_file):Tagged_EHR_File.write(\"\\n\");\n",
        "\n",
        "    if(write_file):Tagged_EHR_File.close();\n",
        "\n",
        "    return np.array(Tagged_EHR_List).transpose();\n",
        "\n",
        "def main():\n",
        "    global Entity_Count;\n",
        "    global Entities_Captured;\n",
        "\n",
        "    Working_dir = \"/content/drive/MyDrive/n2c2/trainingdata_v3/trainingdata_v3/\";\n",
        "    Working_dir = args.working_dir;\n",
        "    max_seq_length = args.max_seq_length;\n",
        "    Training_data_dir = Working_dir + \"train/\";\n",
        "    Eval_data_dir = Working_dir + \"dev/\";\n",
        "\n",
        "    Training_files_list = os.listdir(Training_data_dir);\n",
        "    Eval_files_list = os.listdir(Eval_data_dir);\n",
        "\n",
        "    #Test case\n",
        "    #Generate_cmed_data(EHR_data_dir=Training_data_dir, EHR_fileID=\"336-01\", write_file=True)\n",
        "\n",
        "    #Build training data for Biobert model\n",
        "    cmed_data_dict = {\"Tokens\":[], \"Tags\":[]};\n",
        "    Data_files_dict = {};\n",
        "    for files in Training_files_list:\n",
        "       Data_files_dict[files[:6]] = None;\n",
        "    for IDs in Data_files_dict:\n",
        "        if IDs == '.DS_St':\n",
        "         continue\n",
        "        [tokens, tags] = Generate_cmed_data(EHR_data_dir=Training_data_dir, EHR_fileID=IDs, write_file = False, max_seq_length=max_seq_length);\n",
        "\n",
        "        #if Entity_Count != Entities_Captured:\n",
        "        #    print(\"Entities Not Captured\");\n",
        "        #    print(Entity_Count);\n",
        "        #    print(Entities_Captured);\n",
        "        #    print();\n",
        "\n",
        "        cmed_data_dict[\"Tokens\"].extend(tokens);\n",
        "        cmed_data_dict[\"Tags\"].extend(tags);\n",
        "    cmed_data = pd.DataFrame(cmed_data_dict);\n",
        "    train_data_filename = Working_dir + \"train_dev.tsv\";\n",
        "    cmed_data.to_csv(train_data_filename, sep=\"\\t\",header=False, index = False );\n",
        "\n",
        "    print(\"-------Training Entities Summary--------\")\n",
        "    print(\"Total Entites:\", Entity_Count);\n",
        "    print(\"Entities Captured:\", Entities_Captured);\n",
        "    print(\"Percent Captured:\", np.round(Entities_Captured/Entity_Count, 2));\n",
        "    Entity_Count = 0;\n",
        "    Entities_Captured = 0;\n",
        "\n",
        "    #Build eval data for Biobert model\n",
        "    cmed_data_dict = {\"Tokens\": [], \"Tags\": []};\n",
        "    Data_files_dict = {};\n",
        "    for files in Eval_files_list:\n",
        "       Data_files_dict[files[:6]] = None;\n",
        "    for IDs in Data_files_dict:\n",
        "        if IDs == '.DS_St':\n",
        "          continue\n",
        "        [tokens, tags] = Generate_cmed_data(EHR_data_dir=Eval_data_dir, EHR_fileID=IDs, write_file = False);\n",
        "        cmed_data_dict[\"Tokens\"].extend(tokens);\n",
        "        cmed_data_dict[\"Tags\"].extend(tags);\n",
        "\n",
        "    cmed_data = pd.DataFrame(cmed_data_dict);\n",
        "    eval_data_filename = Working_dir + \"devel.tsv\";\n",
        "    cmed_data.to_csv(eval_data_filename, sep=\"\\t\",header=False, index = False );\n",
        "\n",
        "    test_data_filename = Working_dir + \"test.tsv\";\n",
        "    cmed_data.to_csv(test_data_filename, sep=\"\\t\",header=False, index = False );\n",
        "\n",
        "    print(\"-------Eval Entities Summary--------\")\n",
        "    print(\"Total Entites:\", Entity_Count);\n",
        "    print(\"Entities Captured:\", Entities_Captured);\n",
        "    print(\"Percent Captured:\", np.round(Entities_Captured/Entity_Count, 2));\n",
        "    Entity_Count = 0;\n",
        "    Entities_Captured = 0;\n",
        "\n",
        "    return 0;\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}